# PhÃ¢n tÃ­ch Matmul Instructions - Encoding Conflicts

## ğŸ”´ CÃC Lá»†NH CÃ“ MÃ‚U THUáºªN ENCODING

### NhÃ³m 1: FP8-E5M2 conflicts
```
mfmacc.h.e5:  size_sup=000, s_size=00, d_size=01  (FP8-E5M2 â†’ FP16)
mfmacc.s.e5:  size_sup=000, s_size=00, d_size=10  (FP8-E5M2 â†’ FP32)
```
**Váº¥n Ä‘á»**: KhÃ´ng thá»ƒ phÃ¢n biá»‡t chá»‰ báº±ng size_sup

### NhÃ³m 2: FP8-E4M3 conflicts  
```
mfmacc.h.e4:  size_sup=001, s_size=00, d_size=01  (FP8-E4M3 â†’ FP16)
mfmacc.s.e4:  size_sup=001, s_size=00, d_size=10  (FP8-E4M3 â†’ FP32)
```
**Váº¥n Ä‘á»**: KhÃ´ng thá»ƒ phÃ¢n biá»‡t chá»‰ báº±ng size_sup

### NhÃ³m 3: Double precision conflicts
```
mfmacc.d.s:   size_sup=000, s_size=10, d_size=11  (FP32 â†’ FP64)
mfmacc.s:     size_sup=000, s_size=10, d_size=10  (FP32 â†’ FP32)
```
**Váº¥n Ä‘á»**: CÃ¹ng size_sup=000, s_size=10

## âœ… CÃC Lá»†NH KHÃ”NG CÃ“ MÃ‚U THUáºªN (NÃªn giá»¯ láº¡i)

### Floating-point (rÃµ rÃ ng):
1. âœ… **mfmacc.bf16.e5** - size_sup=100, s_size=00, d_size=01 (FP8-E5M2 â†’ BF16)
2. âœ… **mfmacc.bf16.e4** - size_sup=101, s_size=00, d_size=01 (FP8-E4M3 â†’ BF16)
3. âœ… **mfmacc.h** - size_sup=000, s_size=01, d_size=01 (FP16 â†’ FP16)
4. âœ… **mfmacc.s.h** - size_sup=000, s_size=01, d_size=10 (FP16 â†’ FP32)
5. âœ… **mfmacc.s.bf16** - size_sup=001, s_size=01, d_size=10 (BF16 â†’ FP32)
6. âœ… **mfmacc.s** - size_sup=000, s_size=10, d_size=10 (FP32 â†’ FP32)

### Integer (rÃµ rÃ ng):
7. âœ… **mmacc.w.b** - size_sup=011 (INT8 signedÃ—signed â†’ INT32)
8. âœ… **mmaccu.w.b** - size_sup=000 (INT8 unsignedÃ—unsigned â†’ INT32)
9. âœ… **mmaccus.w.b** - size_sup=001 (INT8 unsignedÃ—signed â†’ INT32)
10. âœ… **mmaccsu.w.b** - size_sup=010 (INT8 signedÃ—unsigned â†’ INT32)

## âŒ CÃC Lá»†NH NÃŠN LOáº I Bá»

### Do mÃ¢u thuáº«n encoding:
- âŒ mfmacc.h.e5, mfmacc.s.e5 (xung Ä‘á»™t vá»›i nhau)
- âŒ mfmacc.h.e4, mfmacc.s.e4 (xung Ä‘á»™t vá»›i nhau)
- âŒ mfmacc.d.s, mfmacc.d (xung Ä‘á»™t vá»›i mfmacc.s, chÆ°a rÃµ rÃ ng)

### Do khÃ´ng cáº§n thiáº¿t cho ML Ä‘Æ¡n giáº£n:
- âŒ mfmacc.s.tf32 (TensorFloat-32 - format Ä‘áº·c biá»‡t, chÆ°a phá»• biáº¿n)
- âŒ mmacc.d.h, mmaccu.d.h, mmaccus.d.h, mmaccsu.d.h (INT16â†’INT64 - Ã­t dÃ¹ng trong ML)
- âŒ Táº¥t cáº£ lá»‡nh packed (pmmacc.*) - format phá»©c táº¡p, khÃ´ng rÃµ spec
- âŒ mmacc.w.bp (bit-packed - khÃ´ng rÃµ format)

## ğŸ“Š ÄÃNH GIÃ TÃC Äá»˜NG Äáº¾N Há»ŒC MÃY

### CÃ¡c lá»‡nh cá»‘t lÃµi cho ML (10 lá»‡nh):
1. âœ… **mfmacc.s** (FP32Ã—FP32â†’FP32) - **QUAN TRá»ŒNG NHáº¤T**
   - Standard precision cho háº§u háº¿t ML workloads
   
2. âœ… **mfmacc.s.h** (FP16Ã—FP16â†’FP32) - **QUAN TRá»ŒNG**
   - Mixed precision training (phá»• biáº¿n trong deep learning)
   
3. âœ… **mfmacc.s.bf16** (BF16Ã—BF16â†’FP32) - **QUAN TRá»ŒNG**
   - BFloat16 training (xu hÆ°á»›ng hiá»‡n Ä‘áº¡i: TPU, A100)
   
4. âœ… **mfmacc.h** (FP16Ã—FP16â†’FP16) - **Há»®U ÃCH**
   - Inference optimization, mobile ML
   
5. âœ… **mfmacc.bf16.e5**, **mfmacc.bf16.e4** - **Há»®U ÃCH**
   - Extreme quantization cho inference (FP8â†’BF16)
   
6. âœ… **mmacc.w.b** vÃ  variants (INT8â†’INT32) - **QUAN TRá»ŒNG**
   - Quantized inference (BERT, ResNet, MobileNet quantized)
   - Edge deployment

### CÃ¡c lá»‡nh bá»‹ loáº¡i bá» - TÃ¡c Ä‘á»™ng:
- âŒ **mfmacc.h.e5/e4, mfmacc.s.e5/e4**: CÃ³ thá»ƒ thay báº±ng bf16.e5/e4
- âŒ **mfmacc.d.s, mfmacc.d**: FP64 Ã­t dÃ¹ng trong ML (chá»‰ dÃ¹ng trong scientific computing)
- âŒ **INT64 operations**: KhÃ´ng cáº§n trong neural networks
- âŒ **TF32**: CÃ³ thá»ƒ dÃ¹ng FP32 thay tháº¿ (Ä‘á»™ chÃ­nh xÃ¡c tÆ°Æ¡ng Ä‘Æ°Æ¡ng cho ML)

## ğŸ¯ Káº¾T LUáº¬N

### CÃ³ thá»ƒ Ã¡p dá»¥ng cho ML Ä‘Æ¡n giáº£n: âœ… **HOÃ€N TOÃ€N CÃ“ THá»‚**

Vá»›i 10 lá»‡nh cÃ²n láº¡i, simulator há»— trá»£ Ä‘á»§ cho:

#### Training:
- âœ… FP32 training (standard)
- âœ… Mixed precision FP16/BF16 training (modern)
- âœ… Full precision accumulation (FP32 accumulator)

#### Inference:
- âœ… FP16 inference (mobile, edge)
- âœ… BF16 inference (cloud)
- âœ… INT8 quantized inference (production)
- âœ… FP8 extreme quantization (experimental)

#### CÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ cháº¡y:
- âœ… CNNs: ResNet, VGG, MobileNet, EfficientNet
- âœ… Transformers: BERT, GPT (small models), ViT
- âœ… RNNs: LSTM, GRU
- âœ… Classical ML: SVM, Linear/Logistic Regression

### KhÃ´ng áº£nh hÆ°á»Ÿng nghiÃªm trá»ng vÃ¬:
1. CÃ¡c lá»‡nh cá»‘t lÃµi (FP32, FP16, BF16, INT8) Ä‘á»u cÃ³
2. Precision widening Ä‘Æ°á»£c há»— trá»£ Ä‘áº§y Ä‘á»§
3. Mixed precision training kháº£ thi
4. Quantized inference kháº£ thi
5. Standard ML frameworks (PyTorch, TensorFlow) chá»§ yáº¿u dÃ¹ng FP32/FP16/BF16/INT8

## ğŸ“ KHUYáº¾N NGHá»Š TRIá»‚N KHAI

### Priority 1 (Báº¯t buá»™c):
1. mfmacc.s (FP32Ã—FP32)
2. mmacc.w.b + variants (INT8 quantized)

### Priority 2 (Ráº¥t nÃªn cÃ³):
3. mfmacc.s.h (FP16â†’FP32)
4. mfmacc.s.bf16 (BF16â†’FP32)
5. mfmacc.h (FP16Ã—FP16)

### Priority 3 (Nice to have):
6. mfmacc.bf16.e5/e4 (FP8 extreme quantization)

### CÃ³ thá»ƒ bá» qua:
- Táº¥t cáº£ lá»‡nh FP64
- Táº¥t cáº£ lá»‡nh INT64
- Táº¥t cáº£ lá»‡nh packed/special format
- TensorFloat-32
